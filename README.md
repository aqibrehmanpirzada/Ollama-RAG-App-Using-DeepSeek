# Ollama Streamlit App for Deepseek

## Overview
This Streamlit app is designed to work with the Ollama model, providing a user-friendly interface for interacting with the model and exploring its capabilities.

## Features
- Text-based input: Users can input text prompts to generate responses from the Ollama model.
- Model selection: Users can select from various Ollama models, each with its own strengths and weaknesses.
- Response visualization: The app displays the model's responses in a clear and readable format.
- Customization options: Users can adjust settings, such as the model's temperature and max tokens, to fine-tune the output.

## Requirements
- Python 3.8+: The app requires Python 3.8 or later to run.
- Streamlit: The app is built using Streamlit, which can be installed using pip.
- Ollama model: The app requires access to the Ollama model, which can be obtained through the Deepseek platform.

## Installation
1. Clone the repository: https://github.com/aqibrehmanpirzada/Ollama-RAG-App-Using-DeepSeek.git
2. Install the required dependencies: pip install -r requirements.txt
3. Run the app: streamlit run app.py

## Usage
1. Open the app in your web browser: http://localhost:8501
2. Input a text prompt in the text area.
3. Select the desired Ollama model from the dropdown menu.
4. Adjust the settings as needed.
5. Click the "Generate" button to receive a response from the model.

## Contributing
Contributions are welcome! If you'd like to contribute to the app, please fork the repository and submit a pull request.

## License
This app is licensed under the MIT License.
